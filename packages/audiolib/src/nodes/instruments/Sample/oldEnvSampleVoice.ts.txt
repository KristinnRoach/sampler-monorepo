import {
  LibVoiceNode,
  VoiceType,
  Messenger,
  Destination,
  Connectable,
} from '@/nodes/LibNode';
import { getAudioContext } from '@/context';
import { createNodeId, NodeID, deleteNodeId } from '@/nodes/node-store';
import { VoiceState } from '../VoiceState';

import {
  Message,
  MessageHandler,
  createMessageBus,
  MessageBus,
} from '@/events';

import { assert, midiToPlaybackRate } from '@/utils';

import {
  // createCustomEnvelope,
  // CustomEnvelope,
  SampleVoiceEnvelopes,
  ENV_DEFAULTS,
} from '@/nodes/params/envelopes';
import { EnvelopeType } from '@/nodes/params/envelopes';

export class SampleVoice implements LibVoiceNode, Connectable, Messenger {
  readonly nodeId: NodeID;
  readonly nodeType: VoiceType = 'sample';

  #destination: Destination | null = null;
  #outputNode: AudioNode;
  #worklet: AudioWorkletNode;
  #messages: MessageBus<Message>;

  #state: VoiceState = VoiceState.NOT_READY;
  #isInitialized: boolean = false;
  #activeMidiNote: number | null = null;
  #startedTimestamp: number = -1;

  #sampleDurationSeconds = 0;
  #playbackDurationNormalized = 0;

  // #envelopes: Map<EnvelopeType, CustomEnvelope> = new Map();

  envelopes: SampleVoiceEnvelopes;

  #hpf: BiquadFilterNode | null = null;
  #lpf: BiquadFilterNode | null = null;

  #filtersEnabled = false;
  #loopEnabled = false;

  #attackSec: number = 0.1; // replaced with envelope (keep for non-env scenarios ?)
  #releaseSec: number = 0.1;

  #hpfHz: number = 100;
  #lpfHz: number;
  #lpfQ: number = 1;
  #hpfQ: number = 1;

  constructor(
    private context: AudioContext = getAudioContext(),
    options: { processorOptions?: any; enableFilters?: boolean } = {}
  ) {
    this.nodeId = createNodeId(this.nodeType);
    this.#messages = createMessageBus<Message>(this.nodeId);

    this.#worklet = new AudioWorkletNode(context, 'sample-player-processor', {
      numberOfInputs: 0,
      numberOfOutputs: 1,
      processorOptions: options.processorOptions || {},
    });

    this.envelopes = new SampleVoiceEnvelopes(context, this.#worklet);
    // this.#envelopes.set(
    //   'amp-env',
    //   createCustomEnvelope({
    //     audioContext: context,
    //     fullDuration: 1,
    //     envelopeType: 'amp-env',
    //     defaultPoints: ENV_DEFAULTS['amp-env'].points,
    //   })
    // );

    // this.#envelopes.set(
    //   'loop-env',
    //   createCustomEnvelope({
    //     audioContext: context,
    //     fullDuration: 1,
    //     envelopeType: 'loop-env',
    //     defaultPoints: ENV_DEFAULTS['loop-env'].points,
    //   })
    // );

    // Set low-pass filter frequency based on context sample rate
    this.#lpfHz = this.context.sampleRate / 2 - 100;
    this.#filtersEnabled = options.enableFilters ?? true;

    // Create filters if enabled
    if (this.#filtersEnabled) {
      this.#hpf = new BiquadFilterNode(context, {
        type: 'highpass',
        frequency: this.#hpfHz,
        Q: this.#hpfQ,
      });
      this.#lpf = new BiquadFilterNode(context, {
        type: 'lowpass',
        frequency: this.#lpfHz,
        Q: this.#lpfQ,
      });

      // Connect chain: worklet → hpf → lpf
      this.#worklet.connect(this.#hpf);
      this.#hpf.connect(this.#lpf);
      // todo: set destination here also ?
      this.#outputNode = this.#lpf;
    } else {
      // No filters, worklet is the output node
      this.#outputNode = this.#worklet;
    }

    this.setupMessageHandling();
    this.sendToProcessor({ type: 'voice:init' });
  }

  logAvailableParams = () => {
    console.table(
      'Available worklet params:',
      Array.from(this.#worklet.parameters.keys())
    );
  };

  protected sendUpstreamMessage(type: string, data: any) {
    this.#messages.sendMessage(type, data);
    return this;
  }

  private setupMessageHandling() {
    this.#worklet.port.onmessage = (event: MessageEvent) => {
      let { type, ...data } = event.data;

      switch (type) {
        case 'initialized':
          this.#isInitialized = true;
          this.#state = VoiceState.NOT_READY; // not loaded
          // this.logAvailableParams(); // as needed for debugging
          break;

        case 'voice:loaded':
          this.#activeMidiNote = null;
          this.#state = VoiceState.LOADED;

          if (data.duration) {
            this.#activeMidiNote = null;
            this.#sampleDurationSeconds = data.duration;
            this.#playbackDurationNormalized = 1;

            this.setStartPoint(0);
            this.setEndPoint(1); // normalized !

            // this.#envelopes.forEach((env) => {
            //   env.maxDuration = data.duration;
            // });

            this.#worklet.parameters.get('loopEnd')!.value = 0; // ! Why can this not be set to 1 ??
          }
          break;

        case 'voice:started':
          this.#state = VoiceState.PLAYING;
          data = { voice: this, midiNote: this.#activeMidiNote };
          break;

        case 'voice:stopped':
          this.#state = VoiceState.STOPPED;
          data = { voice: this, midiNote: this.#activeMidiNote };
          this.#activeMidiNote = null;
          break;

        case 'voice:releasing':
          this.#state = VoiceState.RELEASING;
          data = { voice: this, midiNote: this.#activeMidiNote };
          break;

        case 'loop:enabled':
          this.#loopEnabled = true;
          break;

        case 'voice:looped':
          // console.debug(`voice:looped, time: ${data.timestamp}, loopCount: ${data.count}`);

          // this.#applyEnvelopes(
          //   ['amp-env'],
          //   data.timestamp,
          //   1 // this.getParam('playbackRate')!.value
          // );
          break;

        case 'voice:position':
          this.getParam('playbackPosition')?.setValueAtTime(
            data.position,
            this.context.currentTime
          );
          break;

        case 'debug:params':
          console.debug(
            'Debug params: ',
            { loopStart: data.loopStart },
            { loopStartSamples: data.loopStartSamples },
            { loopEnd: data.loopEnd },
            { loopEndSamples: data.loopEndSamples }
          );
          break;

        default:
          console.warn(`Unhandled message type: ${type}`);
          break;
      }

      this.sendUpstreamMessage(type, data);
    };
  }

  async loadBuffer(
    buffer: AudioBuffer,
    zeroCrossings?: number[]
  ): Promise<boolean> {
    this.#state = VoiceState.NOT_READY;

    if (buffer.sampleRate !== this.context.sampleRate) {
      console.warn(
        `Sample rate mismatch - buffer: ${buffer.sampleRate}, context: ${this.context.sampleRate}`
      );
      return false;
    }

    const bufferData = Array.from({ length: buffer.numberOfChannels }, (_, i) =>
      buffer.getChannelData(i).slice()
    );

    this.sendToProcessor({
      type: 'voice:set_buffer',
      buffer: bufferData,
      duration: buffer.duration,
    });

    if (zeroCrossings?.length) this.#setZeroCrossings(zeroCrossings);

    return true;
  }

  #setZeroCrossings(zeroCrossings: number[]): this {
    this.sendToProcessor({
      type: 'voice:set_zero_crossings',
      zeroCrossings,
    });
    return this;
  }

  trigger(options: {
    midiNote: MidiValue;
    velocity: MidiValue;
    secondsFromNow?: number;
  }): number | string | null | undefined {
    const {
      midiNote = 60,
      velocity = 100,
      secondsFromNow = 0,
    } = {
      ...options,
    };

    // Using the same audio context current time for all ops
    const timestamp = this.now + secondsFromNow;

    this.#startedTimestamp = timestamp;
    this.#activeMidiNote = options.midiNote;

    if (
      this.#state === VoiceState.PLAYING ||
      this.#state === VoiceState.RELEASING
    ) {
      console.log(`had to stop a playing voice, midinote: ${midiNote}`);
      // Clears automations to avoid overlapping scheduling
      // conflicts when re-triggered (setValueCurveAtTime)
      this.stop(timestamp);
      return null;
    }

    this.#state = VoiceState.PLAYING;

    const playbackRate = midiToPlaybackRate(midiNote);

    // setParams ensures all params executed using exact same timestamp
    this.setParams(
      [
        { name: 'playbackRate', value: playbackRate },
        { name: 'velocity', value: velocity },
        // todo: set all other params (e.g. startPoint & endPoint) via param handlers (not in trigger method)
      ],
      timestamp
    );

    // this.#applyEnvelopes(['amp-env'], timestamp, 1); // todo: add 'loop-env'
    this.envelopes.triggerEnvelopes(
      timestamp,
      this.#sampleDurationSeconds,
      playbackRate
    );

    // Start playback
    this.sendToProcessor({
      type: 'voice:start',
      when: timestamp,
    });

    return this.#activeMidiNote;
  }

  debugDuration() {
    console.info(`
      sample duration: ${this.sampleDuration}, 
      startPoint: ${this.getParam('startPoint')!.value}, 
      endPoint: ${this.getParam('endPoint')!.value}, 
      playback duration: ${this.getPlaybackDuration()}
      `);
  }

  release({ release = this.#releaseSec, secondsFromNow = 0 }): this {
    // this.#envelopes.forEach((env) => env.stopLooping());

    if (this.#state === VoiceState.RELEASING) return this;

    const envGain = this.getParam('envGain');
    if (!envGain) throw new Error('Cannot release - envGain parameter is null');

    this.#state = VoiceState.RELEASING;
    const timestamp = this.now + secondsFromNow;

    // Immediate stop for zero release time
    if (release <= 0) return this.stop(timestamp);

    // const currentGainValue = envGain.value;
    // this.releaseEnvelopes('amp-env', timestamp, release, currentGainValue, 0);

    this.envelopes.releaseEnvelopes(timestamp, release);

    this.sendToProcessor({ type: 'voice:release' });

    // After the release duration, the voice should stop
    setTimeout(
      () => {
        if (this.#state === VoiceState.RELEASING) this.stop();
      },
      release * 1000 + 50
    ); // 50ms buffer

    return this;
  }

  stop(timestamp = this.now): this {
    // this.#envelopes.forEach((env) => env.stopLooping()); // idempotent

    if (
      this.#state === VoiceState.STOPPED ||
      this.#state === VoiceState.STOPPING
    ) {
      return this;
    }
    this.#state = VoiceState.STOPPING;

    // Clear all scheduled values to prevent overlapping setValueCurveAtTime errors
    this.setParam('envGain', 0, timestamp, {
      cancelPrevious: true,
      glideTime: 0,
    });

    this.sendToProcessor({ type: 'voice:stop' });
    this.#state = VoiceState.STOPPED;
    return this;
  }

  // === Envelope Methods ===

  // #applyEnvelopes(
  //   envelopes: EnvelopeType[],
  //   timestamp: number,
  //   playbackRate = 1
  // ) {
  //   if (this.#loopEnabled)
  //     playbackRate = this.getParam('playbackRate')?.value ?? 1;

  //   envelopes.forEach((envType) => {
  //     const envelope = this.#envelopes.get(envType);
  //     const paramName = this.#getParamForEnvelope(envType);
  //     const param = this.getParam(paramName);

  //     if (envelope && param) {
  //       this.#applyEnvelopeToParam(envelope, param, timestamp, playbackRate);
  //     }
  //   });
  // }

  #getParamForEnvelope(envType: EnvelopeType): string {
    const mapping = {
      'amp-env': 'envGain',
      'pitch-env': 'playbackRate',
      'loop-env': 'loopEnd', // or whatever
      default: 'envGain',
    };
    return mapping[envType];
  }

  // #applyEnvelopeToParam(
  //   envelope: CustomEnvelope,
  //   audioParam: AudioParam,
  //   startTime: number,
  //   playbackRate: number
  // ) {
  //   audioParam.cancelScheduledValues(startTime);

  //   // Convert normalized to real time
  //   const realDuration =
  //     (this.#sampleDurationSeconds * envelope.normalizedDuration) /
  //     playbackRate;
  //   const realStartTime =
  //     startTime +
  //     (this.#sampleDurationSeconds * envelope.startTime) / playbackRate;

  //   this.#applySingleEnvelope(
  //     envelope,
  //     audioParam,
  //     realStartTime,
  //     realDuration
  //   );
  // }

  // #applySingleEnvelope(
  //   envelope: CustomEnvelope,
  //   audioParam: AudioParam,
  //   startTime: number,
  //   duration: number
  // ) {
  //   const sampleRate = 1000;
  //   const numSamples = Math.max(2, Math.floor(duration * sampleRate));
  //   const curve = new Float32Array(numSamples);

  //   for (let i = 0; i < numSamples; i++) {
  //     const normalizedTime =
  //       (i / (numSamples - 1)) * envelope.normalizedDuration +
  //       envelope.startTime;
  //     curve[i] = Math.max(
  //       envelope.interpolateValueAtTime(normalizedTime),
  //       0.001
  //     );
  //   }

  //   audioParam.setValueCurveAtTime(curve, startTime, duration);
  // }

  // loopEnvelope = (envType: EnvelopeType, loop: boolean) => {
  //   const env = this.#envelopes.get(envType);
  //   env?.setLoopEnabled(loop);
  // };

  // /**
  //  * Applies the release portion of the envelope to an AudioParam.
  //  * This assumes the envelope should decay from its current value to 0 over the releaseDuration.
  //  * It finds the effective starting point for the release within the envelope's definition.
  //  * @param audioParam The AudioParam to schedule.
  //  * @param startTime The Web Audio API timestamp to start the release.
  //  * @param duration The duration of the release phase.
  //  * @param currentValue The actual current value of the audioParam at releaseStartTime.
  //  * @param targetValue The value the envelope should reach at the end of the release. (Usually 0 or minValue)
  //  * @param minValue A minimum value to ensure no complete silence (avoids denormals).
  //  */
  // releaseEnvelopes(
  //   envType: EnvelopeType,
  //   startTime: number,
  //   duration: number, // todo: remove
  //   currentValue: number,
  //   targetValue: number = 0.0001,
  //   minValue: number = 0.0001
  // ) {
  //   const audioParam = this.getParam(this.#getParamForEnvelope(envType));
  //   if (!audioParam) return;

  //   // cancelScheduledParamValues(audioParam, startTime);
  //   audioParam.cancelScheduledValues(startTime);
  //   audioParam.setValueAtTime(audioParam.value, startTime);

  //   const env = this.getEnvelope(envType);
  //   const releaseStartPoint = env.releasePoint;
  //   const endTime = env.endPoint.time;

  //   duration = endTime - releaseStartPoint.time;

  //   const effectiveCurveType = releaseStartPoint.curve || 'exponential';

  //   switch (effectiveCurveType) {
  //     case 'exponential':
  //       // Exponential ramp requires start and end to be > 0.
  //       if (currentValue > minValue && targetValue > 0) {
  //         audioParam.exponentialRampToValueAtTime(
  //           targetValue,
  //           startTime + duration
  //         );
  //       } else {
  //         audioParam.linearRampToValueAtTime(targetValue, startTime + duration);
  //       }
  //       break;
  //     case 'linear':
  //     default:
  //       audioParam.linearRampToValueAtTime(targetValue, startTime + duration);
  //       break;
  //   }
  // }

  addEnvelopePoint(envType: EnvelopeType, time: number, value: number) {
    // const envelope = this.#envelopes.get(envType);
    // if (!envelope) return;

    // envelope.addPoint(time, value);
    // this.#applyEnvelopeIfPlaying(envType);

    this.envelopes.addEnvelopePoint(envType, time, value);
  }

  updateEnvelopePoint(
    envType: EnvelopeType,
    index: number,
    time: number,
    value: number
  ) {
    // const envelope = this.#envelopes.get(envType);
    // if (!envelope) return;
    // envelope.updatePoint(index, time, value);
    // this.#applyEnvelopeIfPlaying(envType);

    this.envelopes.updateEnvelopePoint(envType, index, time, value);
  }

  deleteEnvelopePoint(envType: EnvelopeType, index: number) {
    // const envelope = this.#envelopes.get(envType);
    // if (!envelope) return;
    // envelope.deletePoint(index);
    // this.#applyEnvelopeIfPlaying(envType);

    this.envelopes.deleteEnvelopePoint(envType, index);
  }

  connect(
    destination: Destination,
    output?: number,
    input?: number
  ): Destination {
    if (destination instanceof AudioParam) {
      this.out.connect(destination, output);
    } else if (destination instanceof AudioNode) {
      this.out.connect(destination, output, input);
    } else {
      console.warn(`SampleVoice: Unsupported destination: ${destination}`);
    }
    return destination;
  }

  disconnect(output = 'main', destination?: Destination): this {
    if (output === 'alt') {
      console.warn(`SampleVoice has no "alt" output to disconnect`);
      return this;
    }
    if (!destination) {
      this.out.disconnect();
    } else if (destination instanceof AudioNode) {
      this.out.disconnect(destination);
    } else if (destination instanceof AudioParam) {
      this.out.disconnect(destination);
    }
    return this;
  }

  setParam(
    name: string,
    targetValue: number,
    timestamp: number,
    options: {
      glideTime?: number;
      cancelPrevious?: boolean;
    } = {}
  ): this {
    const param = this.getParam(name);
    if (!param || param.value === targetValue) return this;

    const { glideTime = 0, cancelPrevious = true } = options;

    if (cancelPrevious) param.cancelScheduledValues(timestamp); // cancelScheduledParamValues(param, timestamp, currVal);

    if (glideTime <= 0)
      param.setValueAtTime(targetValue, Math.max(timestamp, this.now + 0.001));
    else
      param.linearRampToValueAtTime(
        targetValue,
        timestamp + Math.max(glideTime, 0.001)
      );

    return this;
  }

  protected setParams(
    paramsAndValues: Array<{ name: string; value: number }>,
    atTime: number,
    options: {
      glideTime?: number;
      cancelPrevious?: boolean;
    } = {}
  ): this {
    const validParams = paramsAndValues.filter(
      (pv) => this.getParam(pv.name) !== null
    );
    if (validParams.length === 0) return this;

    validParams.forEach(({ name, value }) => {
      // Pass the absolute timestamp to ensure all parameters use the same timestamp
      this.setParam(name, value, atTime, { ...options });
    });
    return this;
  }

  setAttack = (attack_sec: number) => {
    this.#attackSec = attack_sec;
    console.info(`setAttack called, to be replaced with envelope`);
  };

  setRelease = (release_sec: number) => {
    this.#releaseSec = release_sec;
    console.info(`setRelease called, to be replaced with envelope`);
  };

  setStartPoint = (point: number, timestamp = this.now) => {
    this.setParam('startPoint', point, timestamp);
  };

  setEndPoint = (point: number, timestamp = this.now) => {
    this.setParam('endPoint', point, timestamp);
  };

  debugCounter = 0;

  setLoopPoints(start: number, end: number, timestamp = this.now): this {
    if (start !== undefined) {
      this.setParam('loopStart', start, timestamp, { glideTime: 0 });
    }
    if (end !== undefined) {
      this.setParam('loopEnd', end, timestamp, { glideTime: 0 });

      // if (this.#envelopes.get('loop-env')?.durationInSeconds) {
      //   this.#envelopes.get('loop-env')!.durationInSeconds = end - start;
      // }
    }

    return this;
  }

  /** MESSAGES */
  sendToProcessor(data: any): this {
    this.#worklet.port.postMessage(data);
    return this;
  }

  onMessage(type: string, handler: MessageHandler<Message>): () => void {
    return this.#messages.onMessage(type, handler);
  }

  // Getters

  getPlaybackDuration() {
    return this.#playbackDurationNormalized;
  }

  // getEnvelope(envType: EnvelopeType): CustomEnvelope {
  //   assert(
  //     this.#envelopes.has(envType),
  //     `SampleVoice does not have envelope of type: ${envType}`,
  //     this.getEnvelope
  //   );

  //   return this.#envelopes.get(envType)!;
  // }

  getEnvelope(envType: EnvelopeType) {
    return this.envelopes.getEnvelope(envType);
  }

  get currMidiNote(): number | null {
    return this.#activeMidiNote;
  }

  get hpf() {
    return this.#hpf;
  }

  get lpf() {
    return this.#lpf;
  }

  get in() {
    return null;
  }

  get out() {
    return this.#outputNode;
  }

  get destination() {
    return this.#destination;
  }

  get state(): VoiceState {
    return this.#state;
  }

  get initialized() {
    return this.#isInitialized;
  }

  get now(): number {
    return this.context.currentTime;
  }

  get activeNoteId(): number | string | null {
    return this.#activeMidiNote;
  }

  get startTime(): number {
    return this.#startedTimestamp;
  }

  get sampleDuration() {
    return this.#sampleDurationSeconds;
  }

  // Setters

  enablePositionTracking(enabled: boolean) {
    this.sendToProcessor({
      type: 'voice:usePlaybackPosition',
      value: enabled,
    });

    return this;
  }

  setLoopEnabled(enabled: boolean): this {
    this.sendToProcessor({
      type: 'setLoopEnabled',
      value: enabled,
    });

    this.#loopEnabled = enabled;

    this.envelopes.setEnvelopeLoopEnabled('amp-env', enabled);
    // this.envelopes.setEnvelopeLoopEnabled('loop-env', enabled);

    return this;
  }

  setPlaybackRate(
    rate: number,
    atTime = this.now,
    options?: {
      glideTime?: number;
      cancelPrevious?: boolean;
    }
  ): this {
    this.setParam('playbackRate', rate, atTime, options);

    // this.#updateEnvelopeDuration(); // ? TEST

    return this;
  }

  dispose(): void {
    this.stop();
    this.disconnect();
    this.#worklet.port.close();
    deleteNodeId(this.nodeId);
  }

  getParam(name: string): AudioParam | null {
    if (this.#worklet && this.#worklet.parameters.has(name)) {
      return this.#worklet.parameters.get(name) ?? null;
    }

    // Special case for filter parameters if they exist
    if (this.#filtersEnabled) {
      switch (name) {
        case 'highpass':
        case 'hpf':
          return this.#hpf?.frequency || null;
        case 'lowpass':
        case 'lpf':
          return this.#lpf?.frequency || null;
        case 'hpfQ':
          return this.#hpf?.Q || null;
        case 'lpfQ':
          return this.#lpf?.Q || null;
      }
    }
    return null;
  }
}

// #applyLoopingEnvelope(
//   envelope: CustomEnvelope,
//   audioParam: AudioParam,
//   startTime: number,
//   cycleDuration: number
// ) {
//   let isLooping = true;
//   let currentCycleStart = startTime;

//   const scheduleNextCycle = () => {
//     if (!isLooping) return;

//     // Generate curve for this cycle
//     const sampleRate = 1000;
//     const numSamples = Math.max(2, Math.floor(cycleDuration * sampleRate));
//     const curve = new Float32Array(numSamples);

//     for (let i = 0; i < numSamples; i++) {
//       const normalizedTime =
//         (i / (numSamples - 1)) * envelope.normalizedDuration +
//         envelope.startTime;
//       curve[i] = Math.max(
//         envelope.interpolateValueAtTime(normalizedTime),
//         0.001
//       );
//     }

//     // Schedule this cycle
//     audioParam.setValueCurveAtTime(curve, currentCycleStart, cycleDuration);

//     // Prepare next cycle
//     currentCycleStart += cycleDuration;
//     const timeUntilNext =
//       (currentCycleStart - this.context.currentTime) * 1000;

//     // Schedule next cycle with buffer time
//     if (timeUntilNext > 0) {
//       setTimeout(scheduleNextCycle, Math.max(timeUntilNext - 50, 0));
//     } else {
//       scheduleNextCycle(); // Schedule immediately if we're behind
//     }
//   };

//   // Start the loop
//   scheduleNextCycle();

//   // Store stop function for cleanup
//   this.#envelopeLoopStopFunction = () => {
//     isLooping = false;
//   };
// }

// // Add this property to SampleVoice class
// #envelopeLoopStopFunction: (() => void) | null = null;

// // Add cleanup method
// #stopEnvelopeLooping(): void {
//   if (this.#envelopeLoopStopFunction) {
//     this.#envelopeLoopStopFunction();
//     this.#envelopeLoopStopFunction = null;
//   }
// }
