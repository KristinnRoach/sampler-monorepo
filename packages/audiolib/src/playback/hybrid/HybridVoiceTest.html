<!DOCTYPE html>
<html>
<head>
    <title>Hybrid Voice Test</title>
    <style>
        body { font-family: sans-serif; margin: 20px; }
        button { padding: 10px; margin: 5px; }
    </style>
</head>
<body>
    <h1>Hybrid Voice Test</h1>
    <div>
        <button id="loadBtn">Load Sample</button>
        <button id="playBtn" disabled>Play</button>
        <button id="stopBtn" disabled>Stop</button>
    </div>
    
    <script>
        // Worklet code as a string to avoid extra files during testing
        const processorCode = `
        class HybridProcessor extends AudioWorkletProcessor {
            constructor() {
                super();
                this.active = false;
                
                this.port.onmessage = (e) => {
                    if (e.data.type === 'activate') {
                        this.active = true;
                        this.port.postMessage({ type: 'status', active: true });
                    } else if (e.data.type === 'deactivate') {
                        this.active = false;
                        this.port.postMessage({ type: 'status', active: false });
                    }
                };
            }
            
            process(inputs, outputs) {
                // Pass-through audio with ability to add processing
                const input = inputs[0];
                const output = outputs[0];
                
                // Simple pass-through for now
                for (let channel = 0; channel < input.length; channel++) {
                    const inputChannel = input[channel];
                    const outputChannel = output[channel];
                    
                    for (let i = 0; i < inputChannel.length; i++) {
                        // Here we could add processing but for testing we just pass through
                        outputChannel[i] = inputChannel[i];
                    }
                }
                
                return true;
            }
        }
        
        registerProcessor('hybrid-processor', HybridProcessor);
        `;
        
        class HybridVoice {
            constructor(context) {
                this.context = context;
                this.processor = null;
                this.buffer = null;
                this.sourceNode = null;
                this.isSetup = false;
            }
            
            async setup() {
                if (this.isSetup) return;
                
                try {
                    // Create blob URL for the processor code
                    const blob = new Blob([processorCode], { type: 'application/javascript' });
                    const url = URL.createObjectURL(blob);
                    
                    // Add the worklet module
                    await this.context.audioWorklet.addModule(url);
                    
                    // Create the processor node
                    this.processor = new AudioWorkletNode(this.context, 'hybrid-processor');
                    this.processor.connect(this.context.destination);
                    
                    // Set up message handling
                    this.processor.port.onmessage = (e) => {
                        console.log('Message from processor:', e.data);
                    };
                    
                    // Cleanup URL
                    URL.revokeObjectURL(url);
                    
                    this.isSetup = true;
                    console.log('Hybrid voice setup complete');
                } catch (err) {
                    console.error('Error setting up hybrid voice:', err);
                }
            }
            
            async loadSample(url) {
                try {
                    const response = await fetch(url);
                    const arrayBuffer = await response.arrayBuffer();
                    this.buffer = await this.context.decodeAudioData(arrayBuffer);
                    console.log('Sample loaded successfully');
                    return true;
                } catch (err) {
                    console.error('Error loading sample:', err);
                    return false;
                }
            }
            
            play() {
                if (!this.isSetup || !this.buffer) return;
                
                // Stop previous playback if any
                if (this.sourceNode) {
                    this.sourceNode.stop();
                    this.sourceNode.disconnect();
                }
                
                // Create a new source node
                this.sourceNode = this.context.createBufferSource();
                this.sourceNode.buffer = this.buffer;
                
                // Connect to the processor instead of directly to destination
                this.sourceNode.connect(this.processor);
                
                // Activate the processor
                this.processor.port.postMessage({ type: 'activate' });
                
                // Start playback
                this.sourceNode.start();
                console.log('Playback started');
            }
            
            stop() {
                if (this.sourceNode) {
                    this.sourceNode.stop();
                    this.sourceNode.disconnect();
                    this.sourceNode = null;
                    
                    // Deactivate the processor
                    this.processor.port.postMessage({ type: 'deactivate' });
                    console.log('Playback stopped');
                }
            }
        }
        
        // Main setup
        document.addEventListener('DOMContentLoaded', async () => {
            const loadBtn = document.getElementById('loadBtn');
            const playBtn = document.getElementById('playBtn');
            const stopBtn = document.getElementById('stopBtn');
            
            let audioContext;
            let hybridVoice;
            
            loadBtn.addEventListener('click', async () => {
                // Create audio context on user interaction to satisfy autoplay policies
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                hybridVoice = new HybridVoice(audioContext);
                
                await hybridVoice.setup();
                
                // Use a test tone if no sample is available
                const sampleLoaded = await createTestBuffer();
                
                if (sampleLoaded) {
                    playBtn.disabled = false;
                    stopBtn.disabled = false;
                    loadBtn.disabled = true;
                }
            });
            
            playBtn.addEventListener('click', () => {
                hybridVoice.play();
            });
            
            stopBtn.addEventListener('click', () => {
                hybridVoice.stop();
            });
            
            // Create a test buffer with a simple sine wave when no sample URL is provided
            async function createTestBuffer() {
                const buffer = audioContext.createBuffer(
                    1,                          // channels
                    audioContext.sampleRate * 2, // 2 seconds
                    audioContext.sampleRate
                );
                
                // Fill the buffer with a sine wave
                const data = buffer.getChannelData(0);
                const frequency = 440; // A4 note
                
                for (let i = 0; i < data.length; i++) {
                    data[i] = Math.sin(2 * Math.PI * frequency * i / audioContext.sampleRate) * 0.5;
                }
                
                hybridVoice.buffer = buffer;
                console.log('Test tone created');
                return true;
            }
        });
    </script>
</body>
</html>
